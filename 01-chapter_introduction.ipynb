{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from \"Reinforcement Learning\" by Sutton amd Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement learning** - learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. \n",
    "The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. \n",
    "\n",
    "\n",
    "Features of reinforcement learning:\n",
    "- *trial-and-error search* - actions may affect only the immediate reward\n",
    "- *delayed reward* - actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.\n",
    "\n",
    "One of the challenges that arise in reinforcement learning, and not in other kinds\n",
    "of learning, is the **trade-off between exploration and exploitation**:\n",
    "- **exploitation** - reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward\n",
    "- **exploration** - to discover such actions, it has to try actions that it has not selected before\n",
    "\n",
    "\n",
    "Elements of reinforcement learning:\n",
    "- **policy** - is a mapping from perceived states of the environment to actions to be taken when in those states (stimulus–response rules or associations)\n",
    "- **reward signal** - defines the goal of a reinforcement learning problem - On each time step, the environment sends to the reinforcement learning agent a single number called the *reward*. The agent’s sole objective is to maximize the total reward it receives over the long run. The reward signal thus defines what are the good and bad events for the agent. The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. \n",
    "- **value function** - specifies what is good in the long run. The *value* of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states.\n",
    "- **model** (optional) - this is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced.\n",
    "\n",
    "Methods for solving reinforcement learning problems:\n",
    "- **model-based** - use models and planning\n",
    "- **model-free** - trial-and-error learners (opposite of planning)\n",
    "\n",
    "**State** - whatever information is available to the agent about its environment\n",
    "\n",
    "-----------------------------\n",
    "## Tic-tac-toe example\n",
    "\n",
    "While we are playing, we change the values of the states in which we find ourselves during the game. We attempt to make them more accurate estimates of the probabilities of winning. To do this, we “back up” the value of the state after each greedy move to the state before the move, as suggested by the arrows in Figure 1.1. More precisely, the current value of the earlier state is updated to be closer to the value of the later state. This can be done by moving the earlier state’s value a fraction of the way toward the value of the later state. If we let $S_{t}$ denote the state before the greedy move, and $S_{t}$+1 the state after the move, then the update to the estimated value of $S_{t}$, denoted $V(S_{t})$, can be written as:\n",
    "\n",
    "$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[V(S_{t+1}-V(S_{t}))]$\n",
    "\n",
    "where $\\alpha$ is a small positive fraction called the **step-size parameter**,which influences the **rate of learning**. \n",
    "\n",
    "This update rule is an example of a **temporal-difference learning** method, so called because its changes are based on a difference, V(S_{t+1}-V(S_{t}) between estimates at two successive times.\n",
    "\n",
    "------------------\n",
    "\n",
    "## History\n",
    "\n",
    "- The class of methods for solving optimal control problems by solving this equation (Bellman equation) came to be known as dynamic programming (Bellman, 1957a)\n",
    "- Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markov decision processes (MDPs). \n",
    "- Ronald Howard (1960) devised the policy iteration method for MDPs. \n",
    "- Dynamic programming is widely considered the only feasible way of solving general stochastic optimal control problems.\n",
    "- Further, the simplest form of dynamic programming is a computation that proceeds backwards in time, making it di?cult to see how it could be involved in a learning process that must proceed in a forward direction.\n",
    "- John Tsitsiklis (1996) - “neurodynamic programming” to refer to the combination of dynamic programming and artificial neural networks.\n",
    "- We define a reinforcement learning method as any e↵ective way of solving reinforcement learning problems, and it is now clear that these problems are closely related to optimal control problems, particularly stochastic optimal control problems such as those formulated as MDPs.\n",
    "-  Accordingly, we must consider the solution methods of optimal control, such as dynamic programming, also to be reinforcement learning methods. Because\n",
    "- Thorndike called this the “Law of Effect” because it describes the effect of reinforcing events on the tendency to select actions. \n",
    "- Pavlov described reinforcement as the strengthening of a pattern of behavior due to an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship with another stimulus or with a response.\n",
    "- Some psychologists extended the idea of reinforcement to include weakening as well as strengthening of behavior, and extended the idea of a reinforcer to include possibly the omission or termination of stimulus.\n",
    "- Alan Turing described a design for a “pleasure-pain system” that worked along the lines of the Law of Effect\n",
    "- Marvin Minsky (1954) discussed computational models of reinforcement learning and described his construction of an analog machine composed of components he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators) meant to resemble modifiable synaptic connections in the brain.\n",
    "- Temporal-difference learning methods are distinctive in being driven by the difference between temporally successive estimates of the same quantity—for example, of the probability of winning in the tic-tac-toe example. This thread is smaller and less distinct than the other two, but it has played a particularly important role in the field, in part because temporal-difference methods seem to be new and unique to reinforcement learning.\n",
    "- The origins of temporal-difference learning are in part in animal learning psychology, in particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulus that has been paired with a primary reinforcer such as food or pain and, as a result, has come to take on similar reinforcing properties. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
