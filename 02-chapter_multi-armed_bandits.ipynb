{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from \"Reinforcement Learning\" by Sutton amd Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that **evaluates** the actions taken rather than **instructs** by giving correct actions.\n",
    "\n",
    "- **Purely evaluative feedback** indicates how good the action taken was, but not whether it was the best or the worst action possible. Depends entirely on the action taken.\n",
    "- **Purely instructive feedback**, on the other hand, indicates the correct action to take, independently of the action actually taken (basis of supervised learning). Independent of the action taken.\n",
    "\n",
    "\n",
    "**Nonassociative settings** --> evaluative feedback problem that we explore is a simple version of the k-armed bandit problem (actions taken in one situation).\n",
    "\n",
    "**Associative settings** --> actions are taken in more than one situation.\n",
    "\n",
    "----------------\n",
    "### A k-armed Bandit Problem\n",
    "\n",
    "- In the k-armed bandit problem, we have a decision-maker or **agent**, who chooses between k different **actions**, and receives a **reward** based on the action he chooses.\n",
    "- You are faced repeatedly with a choice among $k$ different options, or actions.\n",
    "- After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected.\n",
    "- Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
    "\n",
    "Each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the **value of that action**. \n",
    "\n",
    "We denote the action selected on time step $t$ as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action $a$, denoted $q_{*}(a)$, is the **expected reward** given that $a$ is selected:\n",
    "\n",
    "$q_{*}(a) \\doteq E[R_{t} | A_{t}=a]$\n",
    "\n",
    "The goal of the agent is to maximize **expected reward** (argmax).\n",
    "\n",
    "We denote the estimated value of action $a$ at time step $t$ as $Q_{t}(a)$. We would like Qt(a) to be close to $q_{t}(a)$.\n",
    "\n",
    "- If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. --? We call these the **greedy** actions. \n",
    "- When you select one of these actions, we say that you are **exploiting** your current knowledge of the values of the actions. \n",
    "- If instead you select one of the nongreedy actions, then we say you are **exploring**, because this enables you to improve your estimate of the nongreedy action’s value.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value Methods\n",
    "\n",
    "**True value** of an action is the mean reward when that action is selected.\n",
    "\n",
    "$Q_{t} \\doteq \\frac{\\text{sum of rewards when $a$ taken prior to $t$}} {\\text{number of times $a$ taken prior to $t$}} = \\frac{\\sum_{i=1}^{t-a}R_{i} . I_{A_{i}=a}}{\\sum_{i=1}^{t-a}I_{A_{i}=a}}$\n",
    "\n",
    "where $I_{predicate}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_{t}(a)$ as some default value, such as 0. \n",
    "\n",
    "As the denominator goes to infinity, by the law of large numbers, $Q_{t}(a)$ converges to $q_{*}(a)$. We call this the **sample-average method** for estimating action values because each estimate is an average of the sample of relevant rewards. \n",
    "\n",
    "The simplest action selection rule is to select one of the actions with the highest\n",
    "estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one *greedy* action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as\n",
    "\n",
    "$A_{t} \\doteq \\text{argmax } Q_{t}(a)$\n",
    "\n",
    "where $argmax_{a}$ denotes the action a for which the expression that follows is maximized (again, with ties broken arbitrarily). \n",
    "\n",
    "Greedy action selection always **exploits** current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.\n",
    "\n",
    "**$\\epsilon-greedy$** methods - a simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\\epsilon$, instead select randomly from among all the actions with equal probability, independently of the action-value estimates.\n",
    "\n",
    "- An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_{t}(a)$ converge to $q_{*}(a)$. This\n",
    "\n",
    "### Incremental Implementation\n",
    "\n",
    "$Q_{n}\\doteq \\frac{R_{1} + R_{2} + ... + R_{{n-1}}}{n-1}$\n",
    "\n",
    "- The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. \n",
    "- However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. \n",
    "- Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.\n",
    "- It is easy to devise incremental\n",
    "formulas for updating averages with small, constant computation required to process each new reward:\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n} \\sum^{n}_{i=1}R_{i}$\n",
    "\n",
    "This implementation requires memory only for $Q_{n}$ and $n$, and only the small computation for each new reward. This update rule is of a form that occurs frequently throughout this book. The general form is:\n",
    "\n",
    "$NewEstimate \\leftarrow OldEstimate + StepSize[Target - OldEstimate]$\n",
    "\n",
    "- The expression $[Target-OldEstimate]$ is an $error$ in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the $n$-th reward.\n",
    "- Note that the step-size parameter (StepSize) used in the incremental method (2.3) changes from time step to time step. In processing the $n$-th reward for action $a$, the method uses the step-size parameter\n",
    "$\\frac{1}{n}$ . In this book we denote the step-size parameter by $\\alpha$ or, more generally, by $\\alpha_{t}(a)$.\n",
    "\n",
    "**Pseudocode**\n",
    "\n",
    "#### Initialize\n",
    "from a = 1:k:\n",
    "    Q(a) <- 0\n",
    "    N(a) <- 0\n",
    "\n",
    "#### Loop forever:\n",
    "\n",
    "A \\leftarrow { argmax_{a} Q(a) with probability 1-\\epsilon (breaking ties randomly)\n",
    "\n",
    "a random action with probability $\\epsilon$\n",
    "\n",
    "$R \\leftarrow bandit(A)$\n",
    "$N(A) \\leftarrow N(A) + 1$\n",
    "$Q(A) \\leftarrow Q(A) + \\frac{1}{N(A)}[R - Q(A)]$ \n",
    "\n",
    "The function bandit(a) is assumed to take an action and return a corresponding reward.\n",
    "\n",
    "\n",
    "The idea of this **upper confidence bound (UCB)** action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value.\n",
    "\n",
    "\n",
    "Dealing with exploitation-exploration dillema:\n",
    "- optimistic initial values encourage early exploration\n",
    "- upper-confidence bound action selection uses uncertainty in the value estimates to balance exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bandit Algorithms\n",
    "\n",
    "Another way:\n",
    "**Numerical preference** for each action $a$, denoted as $Ht(a)$. \n",
    "- The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. \n",
    "- Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no e↵ect on the action probabilities, which are determined according to a **soft-max** distribution (i.e., Gibbs or Boltzmann distribution) as follows:\n",
    "\n",
    "$Pr{A_{t}=a} \\doteq \\frac{e^{H_{t}(a)}}{\\sum^{k}_{b=1}e^{H_{t}(b)}} \\doteq \\pi_{t}(a)$\n",
    "\n",
    "where here we have also introduced a useful new notation, $\\pi_{t}(a)$, for the probability of taking action $a$ at time $t$. Initially all action preferences are the same (e.g., H1(a) = 0, for all a) so that all actions have an equal probability of being selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associative Search (Contextual Bandits)\n",
    "- in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations.\n",
    "- As an example, suppose there are several di↵erent k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. This would appear to you as a single, nonstationary k-armed bandit task whose true action values change randomly from step to step. You could try using one of the methods described in this chapter that can handle nonstationarity, but unless the true action values change slowly, these methods will not work very well.\n",
    "- Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values).\n",
    "- Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.\n",
    "- **associative search task** - involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to a↵ect the next situation as well as the reward, then we have the full reinforcement learning problem.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Our graphs so far have shown the course of learning over time for each algorithm and parameter setting, to produce a **learning curve** for that algorithm and parameter setting. If we plotted learning curves for all algorithms and all parameter settings, then the graph would be too complex and crowded to make clear comparisons. Instead we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. Figure 2.6 shows this measure for the various bandit algorithms from this chapter, each as a function of its own parameter shown on a single scale on the x-axis. This kind of graph is called a **parameter study**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f634bbd1710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD0hJREFUeJzt3W2MXOV5h/HrrjFdBSgv6wW5GHdNsGgNAkMXlICEADeBQgShEipgYlRMNx+CCm2kYsJLXKFKjpQXgdQiOYHaSAGUJlBQTYktE4mmKq7NS8uCYxGRLSwY29gVGIpVsO9+2ONkBbvs7MyZHc/j6yeNZs7ZM8+5b0b8/fjZM8eRmUiSut9vdboASVI9DHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQ6ZzpPNmjUr+/v7p/OUktT1nn322bczs2+y46Y10Pv7+9m0adN0nlKSul5E/Hcjx7nkIkmFMNAlqRAGuiQVYlrX0CWpUz788ENGRkbYs2dPp0uZUE9PD3PmzGHmzJlNvd9Al3RQGBkZ4YgjjqC/v5+I6HQ5n5CZ7Ny5k5GREebNm9fUGC65SDoo7Nmzh97e3gMyzAEigt7e3pb+BmGgSzpoHKhhvl+r9RnoklQI19AlHZT6l62pdbzhFZfWOl4zDHTB8iNrGuedesaR1BSXXCRpGmzcuJHTTjuNPXv28P7773PKKacwNDRU6zmcoUvSNDjrrLO47LLLuP322/nggw+49tprOfXUU2s9h4EuSdPkzjvv5KyzzqKnp4d77rmn9vFdcpGkabJr1y7ee+89du/e3ZZvrBrokjRNBgcHueuuu1i8eDG33HJL7eO75CLpoDTdlxk+8MADHHLIIVxzzTXs3buXc845h6eeeooLL7ywtnMY6JI0DZYsWcKSJUsAmDFjBhs2bKj9HC65SFIhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEJ42aKkg1Nddxn99Xidv9uoM3RJKoSBLknT4I477uDuu+/+9fZtt91W+w26DHRJmgZLly5l9erVAOzbt4+HH36YxYsX13qOSQM9Ik6IiJ9FxOaIeCkibqr2L4+INyLihepxSa2VSVJB+vv76e3t5fnnn2ft2rWcccYZ9Pb21nqORn4p+hHw9cx8LiKOAJ6NiHXVz76Xmd+utSJJKtQNN9zAqlWreOutt7j++utrH3/SGXpmbs3M56rXu4HNwPG1VyJJhbviiit48skn2bhxIxdddFHt40/pssWI6AfOADYA5wI3RsQSYBOjs/j/qbtASWqLDlxmeOihh3LBBRdw1FFHMWPGjNrHb/iXohFxOPAT4ObMfBe4F/gssBDYCnxngvcNRsSmiNi0Y8eOGkqWpO60b98+nnnmGZYuXdqW8RsK9IiYyWiY/zAzHwHIzG2ZuTcz9wHfB84e772ZuTIzBzJzoK+vr666JamrvPzyy5x00kksWrSI+fPnt+Ucky65REQA9wGbM/O7Y/bPzsyt1eYVwFBbKpSkAixYsIBXX321redoZA39XOArwIsR8UK17xvA1RGxEEhgGPhqWyqUpJpkJqNz1ANTZrb0/kkDPTN/Doz3X+CJls4sSdOop6eHnTt30tvbe0CGemayc+dOenp6mh7Dm3NJOijMmTOHkZERDuSLM3p6epgzZ07T7zfQJR0UZs6cybx58zpdRlt5LxdJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIK4f3QpQNY/7I1bRt7eMWlbRtbneEMXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhJg30iDghIn4WEZsj4qWIuKnaf0xErIuIV6rno9tfriRpIo3M0D8Cvp6ZfwB8DvhaRCwAlgHrM3M+sL7aliR1yKSBnplbM/O56vVuYDNwPHA5sLo6bDXw5XYVKUma3JTW0COiHzgD2AAcl5lbYTT0gWPrLk6S1LiGb58bEYcDPwFuzsx3I6LR9w0CgwBz585tpkZ9muVHdroCSQeIhmboETGT0TD/YWY+Uu3eFhGzq5/PBraP997MXJmZA5k50NfXV0fNkqRxNHKVSwD3AZsz87tjfvQ4cF31+jrgsfrLkyQ1qpEll3OBrwAvRsQL1b5vACuAH0XEUuA14Mr2lChJasSkgZ6ZPwcmWjBfVG85kqRm+U1RSSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYVo+Pa5ksrSv2xN28YeXnFp28bWxJyhS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEJ42aJUg3ZeAig1yhm6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEJMGugRcX9EbI+IoTH7lkfEGxHxQvW4pL1lSpIm08gMfRVw8Tj7v5eZC6vHE/WWJUmaqkkDPTOfBnZNQy2SpBa0soZ+Y0T8V7Ukc3RtFUmSmtJsoN8LfBZYCGwFvjPRgRExGBGbImLTjh07mjydJGkyTQV6Zm7LzL2ZuQ/4PnD2pxy7MjMHMnOgr6+v2TolSZNoKtAjYvaYzSuAoYmOlSRNj0n/xaKIeAg4H5gVESPAN4HzI2IhkMAw8NU21ihJasCkgZ6ZV4+z+7421CJJaoHfFJWkQhjoklSISZdc1CbLj+x0BQed/mVrOl2C1FbO0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoTXoas+dVxbv/yd1seQDlLO0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgrh3RYl1a5/2Zq2jT284tK2jd3tnKFLUiEMdEkqhIEuSYWYNNAj4v6I2B4RQ2P2HRMR6yLiler56PaWKUmaTCMz9FXAxR/btwxYn5nzgfXVtiSpgyYN9Mx8Gtj1sd2XA6ur16uBL9dclyRpippdQz8uM7cCVM/H1leSJKkZbb8OPSIGgUGAuXPntvt006OOf91ekmrW7Ax9W0TMBqiet090YGauzMyBzBzo6+tr8nSSpMk0G+iPA9dVr68DHqunHElSsxq5bPEh4N+BkyNiJCKWAiuAL0TEK8AXqm1JUgdNuoaemVdP8KNFNdciSWqB3xSVpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVIi23z5XmpI6bk28/J3Wx5C6kDN0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQni3RUldpX/ZmraNPbzi0raNPR2coUtSIQx0SSqEgS5JhWhpDT0ihoHdwF7go8wcqKMoSdLU1fFL0Qsy8+0axpEktcAlF0kqRKuBnsDaiHg2IgbrKEiS1JxWl1zOzcw3I+JYYF1E/CIznx57QBX0gwBz585t8XTS5Np5nbJ0IGtphp6Zb1bP24FHgbPHOWZlZg5k5kBfX18rp5MkfYqmAz0iDouII/a/Br4IDNVVmCRpalpZcjkOeDQi9o/zYGY+WUtVkqQpazrQM/NV4PQaa5EktcDLFiWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVoo5/sai7LD+y0xVIUls4Q5ekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFOPguW5QaNNxzTctj9O95sIZKNF36l61p29jDKy5t29j7OUOXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5Jheieyxa9S6IaVMflhlI3coYuSYUw0CWpEAa6JBWipUCPiIsjYktE/DIiltVVlCRp6poO9IiYAfwd8MfAAuDqiFhQV2GSpKlpZYZ+NvDLzHw1M/8PeBi4vJ6yJElT1UqgHw+8PmZ7pNonSeqAVq5Dj3H25ScOihgEBqvN9yJiSwvn7LRZwNudLqIN7KttvtSugQ+A3tqi1L6Ib7XU2+81clArgT4CnDBmew7w5scPysyVwMoWznPAiIhNmTnQ6TrqZl/dp9TeSu0Lpqe3VpZcNgLzI2JeRBwKXAU8Xk9ZkqSpanqGnpkfRcSNwE+BGcD9mflSbZVJkqakpXu5ZOYTwBM11dINilg6God9dZ9Seyu1L5iG3iLzE7/HlCR1Ib/6L0mFMNAnEBHDEfFiRLwQEZuqfcdExLqIeKV6PrrTdTYjIo6KiB9HxC8iYnNEfL7be4uIk6vPav/j3Yi4udv7AoiIv4yIlyJiKCIeioieQvq6qerppYi4udrXlX1FxP0RsT0ihsbsm7CXiLi1umXKloi4qK46DPRPd0FmLhxzqdEyYH1mzgfWV9vd6G7gycz8feB0YDNd3ltmbqk+q4XAHwL/CzxKl/cVEccDfwEMZOapjF6AcBXd39epwJ8z+o3z04EvRcR8urevVcDFH9s3bi/VLVKuAk6p3vP31a1UWpeZPsZ5AMPArI/t2wLMrl7PBrZ0us4m+vod4FdUvz8pqbcxvXwR+LcS+uI338g+htGLGP656q/b+7oS+MGY7TuAv+7mvoB+YGjM9ri9ALcCt4457qfA5+uowRn6xBJYGxHPVt92BTguM7cCVM/Hdqy65p0I7AD+ISKej4gfRMRhlNHbflcBD1Wvu7qvzHwD+DbwGrAVeCcz19LlfQFDwHkR0RsRnwEuYfSLit3e11gT9dK226YY6BM7NzPPZPRukl+LiPM6XVBNDgHOBO7NzDOA9+mev9ZOqvqS22XAP3a6ljpU666XA/OA3wUOi4hrO1tV6zJzM/AtYB3wJPCfwEcdLWr6NHTblGYY6BPIzDer5+2MrsWeDWyLiNkA1fP2zlXYtBFgJDM3VNs/ZjTgS+gNRv8Afi4zt1Xb3d7XHwG/yswdmfkh8AhwDt3fF5l5X2aemZnnAbuAVyigrzEm6qWh26Y0w0AfR0QcFhFH7H/N6JrlEKO3NriuOuw64LHOVNi8zHwLeD0iTq52LQJepoDeKlfzm+UW6P6+XgM+FxGfiYhg9PPaTPf3RUQcWz3PBf6E0c+t6/saY6JeHgeuiojfjoh5wHzgP+o4oV8sGkdEnMjorBxGlygezMy/jYhe4EfAXEb/R7syM3d1qMymRcRC4AfAocCrwJ8x+od7V/dWrcW+DpyYme9U+7r+M4uIvwH+lNElieeBG4DD6f6+/hXoBT4E/ioz13fr5xURDwHnM3q3yG3AN4F/YoJeIuI24HpGP9ObM/NfaqnDQJekMrjkIkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSrE/wPwjrWCLQlExwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size = 100\n",
    "\n",
    "x = np.array(np.random.binomial(200, 0.4, size))\n",
    "y = np.array(np.random.binomial(200, 0.3, size))\n",
    "plt.hist(x)\n",
    "plt.hist(y)\n",
    "plt.legend(['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f634b3eeb00>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q = np.zeros(size)\n",
    "Q[0] = 0.5    \n",
    "options = [x[0], y[0]]\n",
    "base = np.random.choice(options)\n",
    "winner = np.max(options)\n",
    "output = [1 if base == winner else 0][0]\n",
    "\n",
    "\n",
    "for i in range(1,size):\n",
    "      \n",
    "    Q[i] = Q[i-1]+(1/i*(output - Q[i-1]))\n",
    "    base = [x[i] if Q[i] < 0.5 else y[i]][0]\n",
    "    winner = np.max(options)\n",
    "    output = [1 if base == winner else 0][0]\n",
    "\n",
    "plt.plot(Q)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner = np.max(options)\n",
    "winner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
